---
layout: post
title: Deriving the Back-Propagation Algorithm
category: post
---
Chapter 1 of [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) presents a basic logistic network for classifying images of handwritten digits.  Although  the description of the back-propagation algorithm will be coming in the next chapter, I want to go through the experience of deriving it myself rather than just translating the provided python code to julia.  What follows is also inspired by the [UFDL Deep Learning Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial).

### Notation
Consider a multi-layer neural network with $L$ layers, defined in terms of the ${L-1}$ weight matrices $$W=\{W^{(1)},\ldots,W^{(L-1)}\}$$ and biases $$b=\{b^{(1)},\ldots,b^{(l-1)}\}$$.  That is $W^{(l)}$ are the weights for outputs in layer $l$ to inputs in layer $l+1$. The bias $b^{(l)}$ is the bias vector for the neurons in layer $l$.

If we set the input $$ x = a^{(1)}$$, the forward propagation equations are compactly given by

$$ 
\begin{align}
z^{(l+1)} =& W^{(l)}a^{(l)} + b^{(l)}\cr
 a^{(l+1)} &= \sigma( z^{(l+1)})
\end{align} 
$$

Let $$ y = n_{W,b}(x) = a^{(L)}$$ be the output of the network.

### Backpropagation

In order train the network via gradient descent, we need to calculate the derivatives of the cost function with respect to the weights and biases.  For a single training example with input $$x$$ and output $$y$$, we have the cost

$$
J(W,x,y) = \frac{1}{2}\lVert n_{W,b}(x) - y \rVert^2
$$

To calculate the gradient for the weights, $$ \frac{\partial J(W,x,y)}{\partial W^{(l)}_{ij}} $$, we can work backwards from the output layer $$L$$:

$$ 
\begin{align}
\frac{\partial J(W,x,y)}{\partial W^{(l)}_{ij}} &= (n_{W,b}(x)_q - y_q) \frac{\partial n_{W,b}(x)_q}{\partial W^{(l)}_{ij}}\\
&=(a^{(L)}_q - y_q)\frac{\partial a_q^{(L)}}{\partial W^{(l)}_{ij}}\\
&=(a^{(L)}_q - y_q)\frac{\partial \sigma(z^{(L)}_q)}{\partial W^{(l)}_{ij}}\\
&=(a^{(L)}_q - y_q)\sigma'(z^{(L)}_q)\frac{\partial z^{(L)}_q}{\partial W^{(l)}_{ij}}\\
&=\epsilon_q^{(L)}\frac{\partial z^{(L)}_q}{\partial W^{(l)}_{ij}}\\
&=\epsilon_q^{(L)}\frac{\partial (W^{(L-1)}_{qr}a_r^{(L-1)} + b_q^{(L-1)})}{\partial W^{(l)}_{ij}} \label{eq:gen}\\
&=\epsilon_i^{(L)}a_j^{(L-1)}\delta_{(l,L-1)} 
+ \epsilon_q^{(L)}W^{(L-1)}_{qr}\frac{\partial a_r^{(L-1)}}{\partial W^{(l)}_{ij}}
\end{align}
$$

where the the first term in the sum is if the arbitrary $l$ we are considering is actually $L-1$ (there is an implicit sum over $q$ when I went to summation notation in the first equality).  The second term in the sum is the beginning of the recurrence if $l < L-1$.

For $$ \frac{\partial J(W,x,y)}{\partial b_i^{(l)}} $$, the derivation proceeds similarly through \eqref{eq:gen}, which then simplifies to

$$
\begin{align}
\frac{\partial J(W,x,y)}{\partial b_i^{(l)}} &= \epsilon_i^{(L)}\delta_{(l,L-1)} + \epsilon_q^{(l)}W^{(L-1)}_{qr}\frac{\partial a_r^{(L-1)}}{\partial b_i^{(l)}}
\end{align}
$$

More compactly

$$
\begin{align}
\frac{\partial J(W,x,y)}{\partial W^{(l)}_{ij}} &= a_j^{(l)}\epsilon_i^{(l+1)}\\
\frac{\partial J(W,x,y)}{\partial b^{(l)}_{i}} &= \epsilon_i^{(l+1)}\\
\epsilon_i^{(l)} & = \epsilon_q^{(l+1)}W^{(l)}_{qi}\sigma'(z_i^{(l)})\\
\epsilon_i^{(L)} & = (a^{(L)}_i - y_i)\sigma'(z^{(L)}_i)
\end{align}
$$

### Future Notes

1. It seems worth adding the regularization described in the [UFDL Notes](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm).  
2. More sophisticated conjugate gradient or BFGS aren't worth pursing for now  [LeCun et. al, Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf).


