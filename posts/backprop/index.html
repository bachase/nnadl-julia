<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <meta charset="utf-8">
  <title>
    
      Deriving the Back-Propagation Algorithm &ndash;
    
    Neural Networks, Deep Learning and Julia
  </title>

  <meta name="author" content="Neural Networks, Deep Learning and Julia" />
  <meta name="description" content="Learning Julia with some ML tasks" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <link rel="alternate" type="application/rss+xml" href="/atom.xml" />

  <link rel="stylesheet" href="/css/fontawesome.min.css" type="text/css" media="screen, projection" />
  <link rel="stylesheet" href="/css/base.css" type="text/css" media="screen, projection" />
  <link rel="stylesheet" href="/css/pygments.css" type="text/css" />
  <link media="only screen and (max-device-width: 480px)" href="/css/mobile.css" type="text/css" rel="stylesheet" />
  <link media="only screen and (device-width: 768px)" href="/css/mobile.css" type="text/css" rel="stylesheet" />
  <link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>
 
  <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
  <script type="text/javascript" src="/js/application.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
        displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
      //,
      //displayAlign: "left",
      //displayIndent: "2em"
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
</head>

<body>
  <section class="sidebar">

    <section class="name">
      <a href="/">
        <span id="fname">Neural Networks, Deep Learning and Julia</span>
      </a>
    </section>

    <section class="meta">
      <a href="https://github.com/bachase" target="_blank"><i class="icon icon-github"></i></a>
      <a href="/atom.xml"><i class="icon icon-rss"></i></a>
    </section>

    <section class="sections">
      <ul>
        <li><a href="/about.html">about</a></li>
        <li><a href="/">posts</a></li>
      </ul>
    </section>  </section>

  <section class="content">
  <h1>
    <a href="/posts/backprop">Deriving the Back-Propagation Algorithm</a>
  </h1>

  <section class="byline">
    December  7, 2013
  </section>

  <p>Chapter 1 of <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> presents a basic logistic network for classifying images of handwritten digits.  Although  the description of the back-propagation algorithm will be coming in the next chapter, I want to go through the experience of deriving it myself rather than just translating the provided python code to julia.  What follows is also inspired by the <a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial">UFDL Deep Learning Tutorial</a>.</p>

<h3 id="notation">Notation</h3>
<p>Let’s begin with notation for a multi-layer neural network.  What follows is slightly different than both references linked above, but I find it much clearer.</p>

<p>Let <script type="math/tex"> x </script> be the <script type="math/tex">(n-1)</script>-dimensional input to the network. Rather than maintaining a separate bias <script type="math/tex"> b </script> for each neuron, extend the input space with an extra constant input <script type="math/tex"> 1 </script>. That is, the augmented input, or first layer activation, is given by the <script type="math/tex">n</script>-dimensional vector 
<script type="math/tex">% <![CDATA[
 a^{(1)} = \begin{bmatrix} x & 1 \end{bmatrix}^T  %]]></script>. </p>

<p>Likewise, if <script type="math/tex"> \tilde{W^{(l)}}</script> is the weight matrix for the <script type="math/tex">l</script>-th layer of the network with bias <script type="math/tex"> b^{(l)} </script>, then 
we define the extended weight matrix
$$ W^{(l)} = \begin{bmatrix}
  \tilde{W^{(l)}} &amp; b \cr
  0 &amp; 1
\end{bmatrix}$$</p>

<p>Thus the forward propagation equations are compactly given by</p>

<script type="math/tex; mode=display">% <![CDATA[
 
\begin{align}
z^{(l)} =& W^{(l)}a^{(l)} \cr
 a^{(l+1)} &= \sigma( z^{(l)})
\end{align} 
 %]]></script>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>To be continued…</p>


  <!-- TODO: bio here -->
</section>
</body>

</html>
